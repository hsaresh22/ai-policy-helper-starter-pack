# Backend
EMBEDDING_MODEL=local-384
LLM_PROVIDER=stub           # options: stub | openai | ollama
OPENAI_API_KEY=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             # if using OpenAI, set this
OLLAMA_HOST=http://ollama:11434
VECTOR_STORE=qdrant         # qdrant | memory
COLLECTION_NAME=policy_helper
CHUNK_SIZE=700
CHUNK_OVERLAP=80

# Frontend
NEXT_PUBLIC_API_BASE=http://localhost:8000